{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching the Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist=input_data.read_data_sets(\"MNIST_data/\",one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0,\n",
       " 0.015686275,\n",
       " 0.019607844,\n",
       " 0.050980397,\n",
       " 0.07058824,\n",
       " 0.08235294,\n",
       " 0.09019608,\n",
       " 0.098039225,\n",
       " 0.121568635,\n",
       " 0.13333334,\n",
       " 0.14509805,\n",
       " 0.14901961,\n",
       " 0.16078432,\n",
       " 0.18823531,\n",
       " 0.20000002,\n",
       " 0.22352943,\n",
       " 0.2392157,\n",
       " 0.24313727,\n",
       " 0.26666668,\n",
       " 0.27058825,\n",
       " 0.29411766,\n",
       " 0.3019608,\n",
       " 0.32156864,\n",
       " 0.32941177,\n",
       " 0.3372549,\n",
       " 0.34901962,\n",
       " 0.3529412,\n",
       " 0.37647063,\n",
       " 0.3803922,\n",
       " 0.4156863,\n",
       " 0.4431373,\n",
       " 0.45098042,\n",
       " 0.45882356,\n",
       " 0.46274513,\n",
       " 0.4666667,\n",
       " 0.48627454,\n",
       " 0.5019608,\n",
       " 0.5411765,\n",
       " 0.54509807,\n",
       " 0.54901963,\n",
       " 0.5568628,\n",
       " 0.6156863,\n",
       " 0.6509804,\n",
       " 0.65882355,\n",
       " 0.6627451,\n",
       " 0.6901961,\n",
       " 0.73333335,\n",
       " 0.7411765,\n",
       " 0.74509805,\n",
       " 0.7803922,\n",
       " 0.7843138,\n",
       " 0.8078432,\n",
       " 0.81568635,\n",
       " 0.8235295,\n",
       " 0.8352942,\n",
       " 0.8431373,\n",
       " 0.8588236,\n",
       " 0.86274517,\n",
       " 0.8705883,\n",
       " 0.8745099,\n",
       " 0.8862746,\n",
       " 0.89019614,\n",
       " 0.8941177,\n",
       " 0.9058824,\n",
       " 0.9176471,\n",
       " 0.9215687,\n",
       " 0.9333334,\n",
       " 0.93725497,\n",
       " 0.94117653,\n",
       " 0.9450981,\n",
       " 0.9490197,\n",
       " 0.95294124,\n",
       " 0.9607844,\n",
       " 0.96470594,\n",
       " 0.9725491,\n",
       " 0.9803922,\n",
       " 0.9843138,\n",
       " 0.9921569,\n",
       " 0.9960785}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(mnist.train.images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intializing all the constant parameters\n",
    "input_width=28\n",
    "input_height=28\n",
    "input_channels=1                            #input image is 28*28*1\n",
    "input_pixels=784\n",
    "\n",
    "n_conv1=32                                  #no. of units in conv layer 1\n",
    "n_conv2=64\n",
    "\n",
    "conv1_k=5                                   #filter size for conv layer 1\n",
    "conv2_k=5\n",
    "\n",
    "conv1_stride=1                              #stride for conv layer 1\n",
    "conv2_stride=1\n",
    "\n",
    "pool1_k=2                                   #pool size for pooling layer 1\n",
    "pool2_k=2\n",
    "\n",
    "n_hidden=1024                               #no. of units in hidden layer\n",
    "n_output=10                                 #no. of units in output layer(there are 10 classes)\n",
    "\n",
    "input_size_to_hiddenlayer=(input_height//(pool1_k*pool2_k))*(input_width//(pool1_k*pool2_k))*n_conv2\n",
    "#this represents the input size going to hidden layer(from pooling layer 2). input_height and\n",
    "#input_width are getting divided by 2 twice(once for pooling layer 1 once for PL2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights={\"c1\":tf.Variable(tf.random_normal([conv1_k,conv1_k,input_channels,n_conv1])),\n",
    "         \"c2\":tf.Variable(tf.random_normal([conv2_k,conv2_k,n_conv1,n_conv2])),\n",
    "         \"h\":tf.Variable(tf.random_normal([input_size_to_hiddenlayer,n_hidden])),  \n",
    "         \"o\":tf.Variable(tf.random_normal([n_hidden,n_output]))\n",
    "         }\n",
    "bias={\"c1\":tf.Variable(tf.random_normal([n_conv1])),\n",
    "      \"c2\":tf.Variable(tf.random_normal([n_conv2])),\n",
    "      \"h\":tf.Variable(tf.random_normal([n_hidden])),\n",
    "      \"o\":tf.Variable(tf.random_normal([n_output]))\n",
    "     }\n",
    "#we are going to take biases for all layers(except pooling layers obviously)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution(x,weights,bias,stride=1):\n",
    "    \n",
    "    #inbuilt function which gives output of convoultion layer given the filter(weights)\n",
    "    \n",
    "    conv_output=tf.nn.conv2d(x,weights,padding=\"SAME\",strides=[1,stride,stride,1])  \n",
    "    \n",
    "    #the arguments of strides is a list, the first element is 1 if u don't want\n",
    "    #to skip any image(no striding over images, basically stride for images=1),\n",
    "    #we also don't want striding over channels, so last argument is also 1 2nd \n",
    "    #argument represents the stride along height and the 3rd argument represent\n",
    "    #the stride along width.\n",
    "    \n",
    "    #inbuilt function to add biases\n",
    "    conv_output=tf.nn.bias_add(conv_output,bias)\n",
    "    \n",
    "    #applying activation function:\n",
    "    conv_output=tf.nn.relu(conv_output)\n",
    "    \n",
    "    return conv_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooling(x,k=2):                                                                 #K is pool size\n",
    "    return tf.nn.max_pool(x,padding=\"SAME\",ksize=[1,k,k,1],strides=[1,k,k,1])       #just ike conv2d arguments,ksize=window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(x,weights,biases,keep_prob):\n",
    "    #images we get will be 784*1, we need to convert it to 28*28*1(1 represents the no. of channels)\n",
    "    x=tf.reshape(x,shape=[-1,input_height,input_width,input_channels])  #first argument of shape means x.shape[0] (no. of images)\n",
    "    \n",
    "    x_conv1=convolution(x,weights[\"c1\"],bias[\"c1\"],conv1_stride)#this will give us the images after passing through conv layer 1\n",
    "    x_pool1=pooling(x_conv1,pool1_k)                            #images after passing through pooling layer 1\n",
    "    x_conv2=convolution(x_pool1,weights[\"c2\"],bias[\"c2\"],conv2_stride)#this will give us the images after passing through conv layer 2\n",
    "    x_pool2=pooling(x_conv2,pool2_k)                            #images after passing through pooling layer 2\n",
    "    \n",
    "    x_hidden=tf.reshape(x_pool2,shape=[-1,input_size_to_hiddenlayer])        #reshaping images to 1d array for hidden layer\n",
    "    \n",
    "    #Adding the dropout layer:\n",
    "    x_output_before_dropout_layer=tf.nn.relu(tf.add(tf.matmul(x_hidden,weights[\"h\"]),bias[\"h\"])) #applying relu activation function for hidden layer\n",
    "    x_output=tf.nn.dropout(x_output_before_dropout,keep_prob)                                                                                    #after applying dropout layer\n",
    "    \n",
    "    output=tf.add(tf.matmul(x_output,weights[\"o\"]),bias[\"o\"])               #identity activation function\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "ses=tf.Session()\n",
    "#ses.run(tf.global_variables_initializer())         \n",
    "#if u run global_variables intializer her, it would result in \n",
    "#an error later on as u are only initializing global variables \n",
    "#before this cell, however, AdamOptimizer has its own Variables, \n",
    "#which also need to be intialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.placeholder(\"float\",[None,input_pixels])        #we give placeholders so that later on we can put any desired \n",
    "y=tf.placeholder(tf.int32,[None,n_output])           #input here(like train data or test data or validation data)\n",
    "\n",
    "keep_prob=tf.placeholder(\"float\")                    \n",
    "#we are making keep_prob a placeholder as we only want to \n",
    "#dropout during training, but during testing, we don't want \n",
    "#any dropout(thus during training, we will giv keep_prob a \n",
    "#value say 0.8 and during testing we will keep it 1.0(so that \n",
    "#all testing data passes). This will also allow us to change \n",
    "#the keep_prob for trainig later on in the code if we want)\n",
    "\n",
    "pred=cnn(x,weights,bias,keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred,labels=y))      \n",
    "#_v2 is put because of deprecation warning(see by removing _v2)(don't worry if u don't understand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "optimize=optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "ses.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "760527.8340978622\n",
      "24023.986181051732\n",
      "14825.647005804683\n",
      "10198.983971779413\n",
      "10155.817009843886\n",
      "10458.878803368973\n",
      "7581.631994838268\n",
      "7802.71980526032\n",
      "7925.24695124953\n",
      "7315.492102293252\n",
      "5292.536300963599\n",
      "5828.928874166671\n",
      "4165.717502967664\n",
      "3468.582054795027\n",
      "4388.864816717729\n",
      "3071.01848409013\n",
      "2361.3068031206667\n",
      "3928.6884337277324\n",
      "2873.775004928157\n",
      "2021.5280072032153\n",
      "3314.2890032692185\n",
      "2824.3441081284814\n",
      "2643.750886018076\n",
      "1760.6566221107475\n",
      "2039.6279076071355\n",
      "2036.9504055351686\n",
      "2582.3366525556144\n",
      "1885.7656999756719\n",
      "1731.7163763058952\n",
      "2010.0390537490184\n",
      "1742.7571859448872\n",
      "1511.603920751379\n",
      "1993.1389727053386\n",
      "1639.5083270730424\n",
      "2354.685860394624\n",
      "1583.1375458155603\n",
      "1731.2513395837304\n",
      "2144.006169395725\n",
      "1602.1907064692305\n",
      "1723.8053895546857\n",
      "1556.2196072489023\n",
      "1897.1282285518387\n",
      "1570.9283383473753\n",
      "1692.176312802927\n",
      "2579.4086716920137\n",
      "1776.7032322727364\n",
      "1215.4996296328306\n",
      "1902.2291058024753\n",
      "1696.9001075863443\n",
      "1820.1481824219227\n",
      "1675.3447245144844\n",
      "1541.4318955496208\n",
      "1595.4300154982984\n",
      "2169.617539510131\n",
      "1504.6938916021586\n",
      "1708.7840210741851\n",
      "1092.6062736756503\n",
      "2168.3088796777756\n",
      "2532.820862941444\n",
      "1428.7885392904282\n",
      "3203.2749152183533\n",
      "2391.8264620041846\n",
      "758.8886620998383\n",
      "1677.9364028424025\n",
      "2324.629601855278\n",
      "2215.0318710058928\n",
      "2020.1460653543472\n",
      "1521.7785344719887\n",
      "1500.0792562514544\n",
      "2006.0746299028397\n",
      "2563.2504193782806\n",
      "2222.4898484370765\n",
      "2319.64490917325\n",
      "624.4576779007912\n",
      "1741.3475371277111\n",
      "2255.080437238328\n",
      "2603.66686527431\n",
      "1215.5957464054227\n",
      "1145.1525267213583\n",
      "2820.267588406801\n",
      "2931.2150173187256\n",
      "1882.8992810499299\n",
      "1418.2824902257416\n",
      "1519.125248398771\n",
      "1326.766285598278\n",
      "1350.9339842358168\n",
      "2421.225280702114\n",
      "1176.6155834112124\n",
      "2434.9618888497353\n",
      "1944.3688331705332\n",
      "2251.6279279368846\n",
      "2056.7768540382385\n",
      "1909.9974838495255\n",
      "1449.5207440257072\n",
      "2461.6808000802994\n",
      "1628.6607069969177\n",
      "2393.9576165676117\n",
      "2316.4548821179233\n",
      "831.3731495141983\n",
      "2833.015958027405\n"
     ]
    }
   ],
   "source": [
    "#Optimizing using batch gradient descent:        #this will take some hours as there are many weights\n",
    "batch_size=100\n",
    "for iter in range(100):\n",
    "    num_batches=int(mnist.train.images.shape[0]/batch_size)\n",
    "    total_cost=0\n",
    "    for j in range(num_batches):\n",
    "        batch_x,batch_y=mnist.train.next_batch(batch_size)\n",
    "        c, _ = ses.run([cost,optimize], feed_dict={x:batch_x,y:batch_y,keep_prob:0.8})\n",
    "        total_cost+=c\n",
    "    print(total_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9894"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions=tf.argmax(pred,axis=1)\n",
    "true=tf.argmax(y,axis=1)\n",
    "correct_pred=tf.equal(predictions,true)\n",
    "\n",
    "y_pred,y_test,correct_predictions=ses.run([predictions,true,correct_pred],feed_dict={x:mnist.test.images,y:mnist.test.labels\n",
    "                                                                                    ,keep_prob=1.0})\n",
    "\n",
    "correct_predictions.sum()\n",
    "\n",
    "#see that 9894/10000 predicitions are correct"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
